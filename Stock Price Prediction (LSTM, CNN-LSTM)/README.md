# Stock Price Prediction Using Machine Learning:  LSTM & CNN-LSTM 
Siheon Jung<br>
Data science, B.S (engineering)<br>
Pennsylvania State University<br>
State College<br>
spj5294@psu.edu<br>

Abstract — As economic growth occurred around the  world, the stock market also grew accordingly [3]. And  countless investors regardless of age or gender flocked into  the stock market. Investors jump into the stock market to  make profits, but things don't always go as they think,  because stock predictions are challenging for a variety of  reasons. One is that the stock index is influenced by  numerous factors, such as the growth rate of the overall  economy, the price level, bond yields, interest rates, the  country's exchange rate, returns from alternative investment  assets, and the revitalization of the labor market [3][4].  Another thing is that the stock index changes from time to  time due to these factors [4]. To solve this, it is planned to  develop a robust, accurate, and efficient machine learning  model for stock price prediction. Since it is close to  impossible to address and predict all factors that affect the  stock index, stock prices will be predicted using easily  accessible big data containing the open value, close value,  high value, low value, and volume of stocks. Stocks will use  S&P 500 stocks. Here, a faster and more accurate machine  learning model, rather than a human, will be used to verify  the data and predict the stock price. Among several machine  learning models, the LSTM (Long Short-Term Memory)  model, which is useful for predicting stock prices as well as  handling multiple time series data, which is one of RNN  (Recurrent Neural Network), was used [1]. In addition, rather  than just an LSTM model, a CNN-LSTM model was used,  which is a combination of CNN and LSTM model that  complements one of the weaknesses of LSTM, feature  extraction [2]. After validation, the CNN-LSTM model was  compared with the LSTM model using Mean Squared Error  (MSE), Root Mean Squared Error (RMSE), Mean Absolute  Error (MAE) and Correlation Coefficient. As a result, CNN LSTM showed a lot better performance than LSTM. 

**Keywords—Stock, Prediction, Machine Learning, LSTM, CNN. **

### 1. INTRODUCTION 
Historically, the global economy has been steadily  growing [5]. If the magnitude of this growth is not felt,  comparing it with 100 years ago in the 1900s, or even 50  years ago, one can see that there has been tremendous growth.  We live in a world that has been growing steadily, to the  extent that negative growth rates are almost unheard of, even  if the growth rate decreases. Even when facing unforeseen  variables like the IMF financial crisis or the coronavirus  situation, which almost halted consumption, production, and  investment, causing a sharp decline in economic growth rate  (real GDP growth rate), the stock market never stops, and  investors actively buy and sell stocks [6]. This unique  situation of the coronavirus, aside, the stock market has been  growing as the economy grows, and now people of all ages  
are entering the stock market. 

The problem is that predicting stocks is very difficult.  One reason is that the stock index is influenced by many  factors, such as the overall economic growth rate, inflation  level, bond yields, interest rates, the exchange rate of the  country, returns on alternative investment assets, and labor  market activity [7]. Another reason is that decisions must  often be made in the absence of certainty. A scientific attitude  of verifying all data before acting only hinders stock  investment. Additionally, humans verifying and making  judgments on such a vast amount of data is not only  inaccurate but also time-inefficient. There is a well-known  candle chart in the stock market that arranges the daily  opening, closing, high, and low prices in a graphical format.  Originating in Japan, patterns of rising and falling on this  chart have been passed down. G. Morris investigated 88  famous rising-falling candle patterns and found that the  average hit rate was 51%, just 1% better than guessing blindly. 

Then comes the question: Can stock prices be predicted  using big data and artificial intelligence? [8] To put the  conclusion first, it is indeed possible to implement artificial  intelligence that predicts stock prices, but achieving the level  of accuracy we expect is practically impossible [9]. The  biggest reason people have illusions about big data for stock  price prediction is because plausible data exists [10]. We can  easily download stock price data for not only a specific  company but for all companies in the S&P 500 for any  desired period in Excel format. This plausible big data fuels  our desire to predict stock prices. However, stock prices are  too difficult to predict by nature. Simple data on past stock  prices has clear limitations in predicting future stock prices.  There are many specific cases, but there are two main factors  that make predicting any future state of a subject difficult  through machine learning technology, and stock prices have  both characteristics [10]. 

First, the more a prediction subject is influenced by  external factors, the harder it is to predict [11]. In simple  terms, stock prices are not determined by past stock prices  alone. Many factors can influence stock prices, such as a  company's performance, economic conditions, technological  advancements, changes in consumer sentiment, marketing  success, and corporate incidents. However, the data we have  is grossly insufficient to reflect this information. This means  that even if one predicts a rise in stock prices based on past  price movements, a single external variable can burst that  prediction bubble. Recent moves to apply programming  technology to stock trading focus on short-term trading or  automated trading rather than long-term investment for this  reason [12]. 

The second reason why it's virtually impossible to predict  stock prices with big data is because it's ambiguous how  much data should be used [13]. If it's an old company, there might be decades of stock price data available. If this data is  divided into daily intervals, the amount of data increases even  more, and it becomes significantly vast if it's further divided  into finer intervals of hours, minutes, and seconds. However,  there's an ambiguity here. The problem lies in how much  period of data is needed for proper predictions and how far  into the future stock prices can be predicted. A common  misconception about big data is thinking that it's good to  input as much data as possible. In fact, the data we use for  predictions should ideally be as similar as possible to the  actual data and its statistical characteristics. If the statistical  characteristics of the data change over time, the accuracy of  predictions made using even the most sophisticated  techniques will drastically decrease. The difficulty here is  that no one knows when and how the many factors that  influence stock prices will change. In simple terms, it's  impossible to know which period or time of past data will  have characteristics similar to the future. Ignoring this and  proceeding with predictions would lead to disastrous results.  For example, let's say an artificial intelligence is designed to  predict stock prices using last year's data. Last year, the stock  market was on a significant upward trend due to the impact  of COVID-19. But what if there are companies whose stock  prices are falling this year? Efforts would be made to predict  future stock prices by inserting data of the decline, but such  declining cases are data our artificial intelligence has never  learned before. Ultimately, our artificial intelligence would  just randomly produce results for future stock prices.  Therefore, in a word, to develop a machine learning model  for predicting stock prices, solutions that can overcome these  problems are necessary. 

In this work, the Long Short-Term Memory (LSTM)  model, designed to solve the long-term dependency problems  of existing models, allowing for the retention of past  information over long periods, and highly effective in  processing complex time series data, was used [1]. However,  not just LSTM alone but a combined CNN (Convolutional  Neural Network) and LSTM, CNN-LSTM, was utilized.  Regarding the dataset, historical data for any stock from the  past 1 week, 2 weeks, and 1 month will be used after data  preprocessing through the Yahoo Finance API. It will be  evaluated using accuracy, runtime, mean squared error  (MSE), root mean squared error (RMSE), and mean absolute  error (MAE). 

### 2. LITERATURE REVIEW 
Currently, numerous literatures deal with the topic of  stock price prediction using machine learning, but one of the  major shortcomings is their inability to accurately predict  highly dynamic and fast-changing patterns in stock price  movement. [1] One literature deals with Stock Price  Prediction Using Machine Learning and LSTM-Based Deep  Learning Models. They propose an approach of hybrid  modeling for stock price prediction building different  machine learning and deep learning-based models. They use  eight regression models (multivariate linear regression,  multivariate adaptive regression spline (MARS), regression  tree, bootstrap aggregation (Bagging), extreme gradient  boosting (XGBoost), random forest (RF), artificial neural  network (ANN), support vector machine (SVM)) and four  deep learning-based regression models using (LSTM LSTM  model for multi-step forecasting with univariate input data of one week, (ii) LSTM model for multi-step forecasting with  univariate input data of two weeks, (iii) Encoder-decoder  LSTM for multi-step forecasting with univariate input data  for two weeks, and (iv) Encoder-decoder LSTM for multi-step  forecasting with multivariate input data for two weeks). And  after evaluation with product-moment correlation  coefficient and root mean squared error, they show that the  LSTM-based univariate model that uses one-week prior data  as input for predicting the next week’s open value of the  NIFTY 50 time series is the most accurate model.  

However, one another literature propose and address  weaknesses of LSTM. [2] LSTM networks are designed to  recognize and utilize patterns over time, making them well suited for time series data. However, their effectiveness  heavily relies on the quality and relevance of input features.  This is because: first, financial datasets can include a wide  range of potential input features, from historical price data  (open, high, low, close, volume) to macroeconomic  indicators, news sentiment, and more. The high  dimensionality can overwhelm LSTMs, leading to difficulties  in learning if irrelevant features are included; and second,  identifying which features are predictive requires substantial  domain expertise and experimentation. LSTMs do not  inherently perform feature selection; they process whatever  input they are given. If the input features are not carefully  chosen or engineered, the model may learn spurious  correlations, leading to poor generalization on unseen data.  On the other hand, the CNN-LSTM architecture combines the  strengths of CNNs in feature extraction with the ability of  LSTMs to understand sequence dependencies. The  advantages of using a Convolutional Neural Network-Long  Short-Term Memory (CNN-LSTM) model for stock price  prediction, as opposed to solely relying on LSTM, are derived  from the inherent strengths and complementary  functionalities of both CNN and LSTM architectures. This  approach is particularly beneficial for handling the complex,  noisy, and non-linear nature of stock market data. One is that  CNNs are highly efficient in spatial feature extraction from  input data. When applied to stock price prediction, CNN can  efficiently process and identify crucial patterns from  historical price data, including opening price, highest price,  lowest price, closing price, volume, turnover, and more. This  automatic feature extraction is vital for capturing the  underlying trends and patterns in stock market data, which  might not be immediately apparent. Second, combining  CNNs for feature extraction with LSTMs for sequence  prediction leverages the strengths of both architectures,  leading to improved prediction accuracy. The CNN-LSTM  model addresses the limitations of using either CNN or  LSTM alone by providing a more nuanced understanding of  both spatial and temporal dimensions of the data. This  synergy results in higher forecasting accuracy. And lastly, the  use of CNNs for initial feature extraction reduces the  dimensionality of the input data before it is fed into the LSTM.  This preprocessing step can help in reducing the complexity  of the LSTM network, as it has to process a more refined and  relevant set of features. Consequently, this can lead to faster  training times and lower computational costs, while still  maintaining or even improving prediction performance.

### 3. METHODOLOGY 
**3.1 Data Collection.** To ensure stock price prediction is  effective, it's essential to incorporate up-to-date data [14].  Therefore, for data collection, data is collected from Yahoo  Finance using API. The dataset includes stock info: open,  high, low, close, adjusted close, volume, dividends, and stock  split. The library, yfinance, was used to fetch historical stock  price data for a specified company and period. And for the  future works, it is able to ensure that the company symbol is  valid by checking against a list of S&P 500 companies  fetched from Wikipedia. 

**3.2 Data Preprocessing.** The raw stock market data for a  specified company over a given period is retrieved from  Yahoo Finance undergoes initial cleaning where unnecessary  columns such as Dividends and Stock Splits are removed.  The data is then reset and reorganized to ensure it contains  only the relevant columns for analysis. Following this, the  Close price data is normalized using the MinMaxScaler from  Scikit-learn, which scales the prices to a range between 0 and  1. This scaling is crucial for stabilizing the neural network's  training process by ensuring consistent numerical ranges,  thus facilitating more effective learning and preventing issues  like exploding gradients. This preprocessing stage sets up the  data for the subsequent steps where it will be transformed into  sequences that serve as inputs for training the LSTM model,  aiming to forecast future stock prices. 

**3.3 Feature Engineering.** Here, the feature engineering  process is designed to prepare historical stock price data for  training an LSTM model. The core of the feature engineering  is the creation of sequential data that the LSTM can process.  For example, by setting prediction_days to 40, the script  constructs sequences where each sequence contains 40  consecutive days of scaled closing prices. These sequences,  stored in x_train, serve as the input features for the  LSTM/CNN-LSTM. Correspondingly, the closing price  immediately following each sequence is captured in y_train,  representing the target output for the model. This process  effectively transforms the time-series data into a supervised  learning format, enabling the LSTM to learn from past price  trends to predict future prices accurately. 

**3.4 Parameter Testing.** First, two different machine  learning models, LSTM and CNN-LSTM, are designed. As  mentioned above, there are no huge difference between the  codes; Only convolutional layers are added before the LSTM  layers. However, adding those layers, or CNN-LSTM does  not always result in better performance. It even led to a  greater MSE, RMSE, and MAE at first, which means lower  accuracy. Therefore, we have gone through hyper parameter  tuning. As mentioned, in the field of stock price prediction,  time period of dataset matters. Hence, time period of dataset  isset to 3 months, which is considered to be short but contains  sufficient data for the prediction. Here, time period of the  entire dataset and days used for prediction are different.  

The parameter for days used for prediction (prediction_days) in time series models like LSTM or CNN LSTM plays a crucial role in determining model performance  by defining the number of time steps used as input [15]. This  directly influences the input shape and complexity of the model. A larger prediction_days allows the model to capture  longer-term dependencies, potentially enhancing its ability to  discern underlying patterns in the data, but it also increases  the risk of overfitting, especially in smaller datasets.  Additionally, a greater number of time steps can reduce the  total number of available training samples and complicate the  model, necessitating more parameters and potentially leading  to longer training times [16]. Choosing the optimal  prediction_days is thus a balancing act that depends heavily  on the specific characteristics and dynamics of the dataset.  Hence, prediction_days is set to 10, 20, and 40 days. 

Finding the best number of units for LSTM layers (or any  type of neural network layer) is an empirical process, heavily  reliant on experimentation, as there's no formula to calculate  the optimal number. The best number of units depends on the  complexity of the task, the volume and variety of the data,  and the specific characteristics of the data itself. We have  used one strategy, grid search [17]. It involves training  multiple models with different configurations and comparing  their performance. For example, we train models with 32, 64,  128, 256 units, etc., and evaluate each model's performance  on our validation set. This method is thorough but can be very  time-consuming, especially for large models and datasets.  

Dropout is a regularization technique to prevent  overfitting by randomly setting a fraction of input units to 0  at each update during training time. The value 0.2 means 20%  of the units will be randomly dropped. Adjusting the dropout  rate can help if the model is overfitting (increase dropout) or  underfitting (decrease dropout) [18]. If the model already  generalizes well to new data, the current rate might be  appropriate. Otherwise, experiment with this value to find a  better balance between model complexity and generalization  ability. Hence, we have set it to 0.1 and 0.2. 

In CNN-LSTM models, the conv_filters parameter,  which determines the number of convolutional filters in a  layer, is crucial for model performance due to its direct  impact on the model's ability to extract features from input  data. Each filter captures specific patterns, and having a  greater number of filters allows the model to detect a wider  variety of features. This diversity in feature detection can be  beneficial for complex problems, as it enables the model to  capture detailed and nuanced patterns, potentially improving  accuracy in tasks involving complex inputs like audio, video,  or intricate time-series data [19]. However, increasing the  number of convolutional filters also increases the model's  complexity, which can lead to longer training times and  higher computational costs. More importantly, a higher  number of filters can increase the risk of overfitting,  particularly if the training data is not sufficiently diverse or  large. Overfitting results in a model that performs well on  training data but poorly on unseen data, as it learns to  recognize noise and random fluctuations as part of the  underlying pattern. Hence, in the s it is set to 32, 64, 128, and  256.  

In neural networks, particularly in CNN-LSTM  architectures, the dense_units parameter, which indicates the  number of neurons in a dense layer, significantly influences  model performance. These dense layers are essential for  synthesizing the features extracted by preceding layers to  make informed predictions. Having a higher number of  neurons enhances the layer's ability to handle complex  patterns, allowing for a more intricate representation of interactions between input features [20]. This increase in  neurons boosts the model's capacity to solve complex  problems by providing a richer set of feature combinations to  learn from. However, a larger number of dense units can also  lead to challenges such as overfitting. This risk necessitates  careful management through regularization techniques and  model validation. Additionally, more neurons mean more  computational resources are needed, which can extend  training times and increase the complexity of the model's  optimization landscape, potentially making it harder for  training algorithms to converge to the best solution. Accordingly, the variable, dense unit, is set to 50 and 100. 

The number of epochs during neural network training,  including LSTM models, plays a crucial role in determining  model accuracy and its ability to generalize. Training for too  few epochs may lead to underfitting, where the model doesn't  learn enough from the data to make accurate predictions. An  optimal number of epochs, on the other hand, allows the  model to adequately learn from the training data and  generalize well to unseen data, achieving a balance between the computational advantages of larger batches and the  improved generalization and faster convergence associated  with smaller batches. The choice of optimal batch size is  typically empirical, depending on the dataset, model  architecture, and available computational resources, and  sometimes involves dynamic adjustments throughout the  training process to maximize efficiency and model  performance. Hence, we have set it to 32, 64, and 128.  

<img width="474" height="383" alt="Image" src="https://github.com/user-attachments/assets/460cffc0-6641-46c4-ae65-436ede332bb1" />

The batch size in neural network training profoundly influences model accuracy, learning dynamics, and computational efficiency. Smaller batch sizes introduce more noise into the training process, acting as a form of regularization that can enhance generalization and lead to faster convergence, albeit with less stable gradient estimates [21]. On the other hand, larger batch sizes provide more accurate gradient calculations, resulting in more stable but potentially slower training progress, and might lead to poorer generalization due to the optimization favoring sharp minima [22]. Intermediate batch sizes aim to strike a balance between the computational advantages of larger batches and the improved generalization and faster convergence associated with smaller batches. The choice of optimal batch size is typically empirical, depending on the dataset, model architecture, and available computational resources, and sometimes involves dynamic adjustments throughout the training process to maximize efficiency and model performance. Hence, we have set it to 32, 64, and 128.

To test these parameters, we have designed a loop  function which is to compare the results (MAE) and find the  best parameters. As a result of parameter testing for LSTM  model, it showed {'units': 128, 'dropout': 0.2, 'batch_size': 32,  'prediction_days': 40} with MAE: 0.03021. And for CNN-LSTM, it showed {'conv_filters': 128, 'lstm_units': 64,  'dense_units': 50, 'dropout_rate': 0.2, 'batch_size': 32,  'prediction_days': 10} with MAE: 0.0360. And then, the  codes for LSTM and CNN-LSTM are modified accordingly. 

<img width="526" height="604" alt="Image" src="https://github.com/user-attachments/assets/71955a47-27fb-4391-8ca3-011792d0360c" />

**3.5 Model Training & Evaluation.** The model training  section involves constructing and training LSTM and CNN LSTM networks using TensorFlow and Keras, tailored for  time series forecasting of stock prices. The LSTM model  comprises several layers including LSTM layers for learning  time-dependent patterns, Dropout layers to mitigate  overfitting, and a Dense layer for outputting the prediction.  For CNN-LSTM, model's architecture comprises Conv1D  layers for feature extraction from sequential data,  MaxPooling1D layers for dimensionality reduction, and  LSTM layers for capturing long-term dependencies in time  series data. Additionally, Flatten and Dense layers interpret  the features, and Dropout layers mitigate overfitting. The  models are compiled using mean squared error as the loss  function and an Adam optimizer for efficient learning. Data  preparation entails creating sequences of prediction_days of  scaled closing stock prices, used as inputs to predict the subsequent day's price. This data is then split into training and  validation sets, with the standard split being approximately  70-80% for training and 20-30% for validation. The training  process involves configuring epochs and batch sizes, and it  utilizes callbacks such as ModelCheckpoint for saving the  model and EarlyStopping to halt training when improvements  cease, enhancing model robustness against overfitting. After  training, the model is evaluated using metrics such as Mean  Squared Error (MSE), Root Mean Squared Error (RMSE),  Mean Absolute Error (MAE), and Correlation Coefficient to  assess accuracy and performance, making this architecture  effective for detailed time series analysis where both  immediate and historical data trends are crucial. 

In order to evaluate the performance of LSTM and CNN LSTM, Mean Squared Error (MSE), Root Mean Squared  Error (RMSE), Mean Absolute Error (MAE), and Correlation  Coefficient are used as evaluation metrics [23] [1]. 
<img width="393" height="372" alt="Image" src="https://github.com/user-attachments/assets/bbda8318-eec0-44ba-951d-4bc104098ac9" />

learning patterns and not memorizing noise. Training for too  many epochs can cause overfitting, where the model learns  the noise in the training data at the expense of its ability to  generalize, resulting in a decline in validation accuracy.  Finding the right number of epochs is a matter of  experimentation and balance, often aided by techniques like  early stopping, which halts training when validation  performance begins to worsen, ensuring the model is neither  underfit nor overfitted. In this case, as number of epochs  

### 4. RESULTS 
Table 3: MSE, RMSE, MAE, and Correlation Coefficient for LSTM and  CNN-LSTM. 
LSTM CNN-LSTM 
MSE 822.8722 147.7533 RMSE 28.6857 12.1554 MAE 23.1519 6.6269 
FIGURE III: Graph of Actual and Predicted Prices by CNN-LSTM 
The metrics shown in Table 3 for LSTM and CNN-LSTM  models demonstrate a significant performance difference  between the two. The LSTM model shows a high Mean  Squared Error (MSE) of 822.8722 and a Root Mean Squared  Error (RMSE) of 28.6857, indicating substantial prediction  errors. Additionally, its Mean Absolute Error (MAE) is  23.11519, and it exhibits a weak negative correlation  coefficient of -0.3835, suggesting its predictions are inversely  related to actual trends, which is generally undesirable. In  contrast, the CNN-LSTM model markedly outperforms the  LSTM with much lower error metrics: an MSE of 147.7533,  an RMSE of 12.1554, and an MAE of 6.6269. It also shows  a moderate positive correlation coefficient of 0.5726,  indicating that its predictions are more aligned with the actual  data trends. This analysis clearly highlights that the CNN LSTM model is superior for this dataset, capturing spatial or  temporal patterns more effectively than the LSTM model. 
5. CONCLUSIONS 
In conclusion, this research presents a significant step  forward in the application of machine learning techniques for  stock price prediction, specifically through the utilization of  LSTM and CNN-LSTM models. The comparative analysis  

Correlation  Coefficient 
-0.3835 0.5726 
reveals that while traditional LSTM models offer certain  benefits, the hybrid CNN-LSTM architecture demonstrates  

FIGURE II: Graph of Actual and Predicted Prices by LSTM 
superior capability in capturing complex temporal and spatial  patterns in stock data, which translates to more accurate  forecasts. This study underscores the importance of feature  extraction and the handling of sequence dependencies in  predicting stock prices. Moreover, the findings suggest that  enhancing machine learning models with CNN layers can  mitigate some of the inherent challenges posed by LSTM's  sensitivity to input feature relevance. These advancements  not only aid in better understanding the dynamic nature of  financial markets but also provide a promising direction for  future research. By focusing on refining these models and  exploring additional hybrid approaches, further  improvements in prediction accuracy and computational  efficiency can be achieved, thus offering valuable tools for  investors and analysts aiming to navigate the increasingly  complex world of stock trading.
6. FUTURE WORKS 
Building on the current research, there are several  avenues for future work that can potentially enhance the  accuracy and robustness of stock price prediction models.  
Current models primarily utilize standard stock market  data such as opening, high, low, close prices, and volume.  Future models could benefit from incorporating additional  variables that influence stock prices. These may include  macroeconomic indicators such as GDP growth rates,  inflation data, and employment rates, as well as  microeconomic factors like company earnings reports,  dividend payouts, and sector performance. Including such  variables could provide a more holistic view of the factors  influencing stock prices. 
While the CNN-LSTM model shows promise, exploring  other complex architectures could yield better results. For  instance, Transformer-based models, which have shown  remarkable success in various domains of natural language  processing, could be adapted for time series forecasting.  These models' ability to handle long-range dependencies  could be particularly beneficial for stock price prediction. 
By addressing these aspects, future research can  significantly advance the field of stock price prediction,  offering more accurate, reliable, and comprehensive tools for  market analysis and decision-making. 
REFERENCES 
[1] Mehtab, S., Sen, J., Dutta, A. (2021). Stock Price Prediction Using  Machine Learning and LSTM-Based Deep Learning Models. In:  Thampi, S.M., Piramuthu, S., Li, KC., Berretti, S., Wozniak, M.,  Singh, D. (eds) Machine Learning and Metaheuristics Algorithms,  and Applications. SoMMA 2020. Communications in Computer and  Information Science, vol 1366. Springer, Singapore.  
https://doi.org/10.1007/978-981-16-0419-5_8 
[2] Lu, W., Li, J., Li, Y., Sun, A., & Wang, J. (2020). A CNN-LSTM Based Model to Forecast Stock Prices. , Article ID  
performance analysis and discussion of implications. International  Journal of Financial Studies, 11(3), 94.  
https://doi.org/10.3390/ijfs11030094 
[9] Awad, A. L., Mesbah, S., & Fakhr, M. W. (2023). Stock market  prediction using deep reinforcement learning. Applied System  Innovation, 6(6), 106. https://doi.org/10.3390/asi6060106 
[10] Cohen, G. (2022). Algorithmic trading and financial forecasting using  advanced artificial intelligence methodologies. Mathematics, 10(18),  3302. https://doi.org/10.3390/math10183302 
[11] Zhong, X., & Enke, D. (2019). Predicting the daily return direction of  the stock market using hybrid machine learning algorithms. Financial  Innovation, 5(1). https://doi.org/10.1186/s40854-019-0138-0 
[12] Kirilenko, A., Kyle, A. S., Samadi, M., & Tuzun, T. (2011). The  Flash crash: The impact of high frequency trading on an electronic  market. Social Science Research Network.  
https://doi.org/10.2139/ssrn.1686004 
[13] Shen, J., & Shafiq, O. (2020). Short-term stock market price trend  prediction using a comprehensive deep learning system. Journal of  Big Data, 7(1). https://doi.org/10.1186/s40537-020-00333-6 [14] AjNavneet. (2023.). GitHub - 
AjNavneet/StockPricePrediction_YFinance_LSTM_RNN: Enhanced  stock prices forecasting using Yfinance, LSTM and RNN. GitHub.  https://github.com/AjNavneet/StockPricePrediction_YFinance_LST M_RNN 
[15] Lim, S., Huh, J., Hong, S., Park, C., & Kim, J. (2022). Solar power  forecasting using CNN-LSTM hybrid model. Energies, 15(21), 8233.  https://doi.org/10.3390/en15218233 
[16] Predicting Stock Market time-series data using CNN-LSTM Neural  Network model. (2023, Feb 29.). Ar5iv.  
https://ar5iv.labs.arxiv.org/html/2305.14378 
[17] Brownlee, J. (2019, Aug 6). How to configure the number of layers  and nodes in a neural network. MachineLearningMastery.com.  https://machinelearningmastery.com/how-to-configure-the-number of-layers-and-nodes-in-a-neural-network/ 
[18] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &  Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural  Networks from Overfitting.  
https://jmlr.org/papers/v15/srivastava14a.html 
[19] Brownlee, J. (2020, Apr 16). How do convolutional layers work in  deep learning neural networks? MachineLearningMastery.com.  https://machinelearningmastery.com/convolutional-layers-for-deep learning-neural-networks/ 
[20] Shiri, F. M., Perumal, T., Mustapha, N., & Mohamed, R. (2023, May  

6622927. 
Complexity, 2020



27). A comprehensive overview and comparative analysis on deep  


https://doi.org/10.1155/2020/6622927



[3] Levine, R., & Zervos, S. (1996). Stock market development and  Long-Run growth. The World Bank Economic Review, 10(2), 323– 339. https://doi.org/10.1093/wber/10.2.323 
[4] Alshubiri, F. (2021). The stock market capitalisation and financial  growth nexus: an empirical study of western European countries.  Future Business Journal, 7(1). https://doi.org/10.1186/s43093-021- 00092-7 
[5] Home. (n.d.). https://www.oecd-ilibrary.org/sites/5fa1c6e1- en/index.html?itemId=/content/component/5fa1c6e1-en 
[6] Ortiz-Ospina, E., Beltekian, D., & Roser, M. (2023, Dec 28). Trade  and globalization. Our World in Data.  
https://ourworldindata.org/trade-and-globalization 
[7] Roser, M., Arriagada, P., Hasell, J., Ritchie, H., & Ortiz-Ospina, E.  (2023, Dec 28). Economic growth. Our World in Data.  
https://ourworldindata.org/economic-growth 
[8] Sonkavde, G., Dharrao, D., Bongale, A. M., Deokate, S. T., Deepak,  D., & Bhat, S. K. (2023). Forecasting stock market prices using  machine learning and deep learning models: a systematic review,  
learning models: CNN, RNN, LSTM, GRU. arXiv.org.  
https://arxiv.org/abs/2305.17473v2 
[21] Understanding Neural Network Batch Training: A Tutorial -- Visual  Studio Magazine. (2014, Aug 1). Visual Studio Magazine.  https://visualstudiomagazine.com/articles/2014/08/01/batch training.aspx 
[22] Brownlee, J. (2020, Aug 27). How to control the stability of training  neural networks with the batch size. MachineLearningMastery.com.  https://machinelearningmastery.com/how-to-control-the-speed-and stability-of-training-neural-networks-with-gradient-descent-batch size/ 
[23] Jailani, N. L. M., Dhanasegaran, J. K., Alkawsi, G., Alkahtani, A. A.,  Phing, C. C., Baashar, Y., Capretz, L. F., Al-Shetwi, A. Q., & Tiong,  S. K. (2023, Mar 28). Investigating the power of LSTM-Based  models in solar energy forecasting. Processes, 11(5), 1382.  https://doi.org/10.3390/pr11051382
